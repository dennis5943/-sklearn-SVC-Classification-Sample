{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n"
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "\n",
    "str = '國民黨7日開中常會，談國民黨主席是否要提早交接，但未果。即將退位的國民黨主席洪秀柱允諾，下周會定案，但她在會後向媒體表示，她不知道她會不會提前請辭，但是她不會默默地走，一定要讓大家知道。中常會結束後，媒體詢問洪秀柱，若吳敦義堅持8月20日召開全代會再交接該怎麼辦，她語氣堅定地說，「他不提前交接，我就提前走」，她還透露國民黨團總召廖國棟，「天天要我滾」。至於下周是否會提前請辭，她不置可否，但是她表示，她不會默默地走，一定會讓大家知道。國民黨主席5月20日已完成改選，黨主席當選人吳敦義陣營與黨主席洪秀柱陣營為了是否提前交接及中央委員提名權、中央委員與中常委選舉是否延到8月20日全代會後等議題，爭執不休。國民黨中常委李德維等26名中常委在7日中常會提案，依黨章規定，將7月8日中央委員選舉、7月29日中常委選舉延到全代會之後舉行，避免違法爭議，進而影響選舉效力。洪秀柱親自主持會議迎戰。'\n",
    "\n",
    "print(str.count('國民黨'))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "迎合|度|100|%|！|超高|級|「|20|歲|出租|女友|」|美乳|好|迷人|！|看|她|忘情|的|扭腰|畫面|太|刺激|了|…| |文| |/| |深夜|大主廚| |在|台灣|，|我們|經常在網|路上|看到|一些|什麼|出租|女友|、|一日|情人|伴遊|等廣\n",
      "女友,出租,100,20,美乳,畫面,大主廚,台灣,我們,經常在網\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "from bs4 import BeautifulSoup\n",
    "import queue\n",
    "import threading\n",
    "import time\n",
    "import datetime\n",
    "import jieba\n",
    "import jieba.analyse\n",
    "from urllib.parse import quote\n",
    "\n",
    "#jieba.load_userdict('userdict.txt')\n",
    "\n",
    "str = \"迎合度100%！超高級「20歲出租女友」美乳好迷人！看她忘情的扭腰畫面太刺激了… 文 / 深夜大主廚 在台灣，我們經常在網路上看到一些什麼出租女友、一日情人伴遊等廣\"\n",
    "\n",
    "seg_list = jieba.cut_for_search(str)\n",
    "tags = jieba.analyse.extract_tags(str, 10)\n",
    "print(\"|\".join(seg_list))\n",
    "print(\",\".join(tags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'泡泡浴': 2, 'TAE': 2, '粉紅': 2, '現場': 2, '成人': 2, '最猛': 1, '畫面': 1, '沒有': 1, '極限': 1, '大主廚': 1, '經過': 0, '粉絲': 0, '連續': 0, '開場': 0, '2017': 0, '女帝': 0}\n",
      "{'泡泡浴': 0, 'TAE': 2, '粉紅': 0, '現場': 0, '成人': 2, '最猛': 0, '畫面': 1, '沒有': 0, '極限': 0, '大主廚': 1, '經過': 1, '粉絲': 1, '連續': 1, '開場': 1, '2017': 1, '女帝': 1}\n",
      "0.5\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "from bs4 import BeautifulSoup\n",
    "import queue\n",
    "import threading\n",
    "import time\n",
    "import datetime\n",
    "import jieba\n",
    "import jieba.analyse\n",
    "from urllib.parse import quote\n",
    "\n",
    "from sklearn.metrics import pairwise_distances\n",
    "from scipy.spatial.distance import cosine\n",
    "from scipy.spatial import distance\n",
    "import numpy as np\n",
    "\n",
    "#jieba.load_userdict('userdict.txt')\n",
    "\n",
    "def StrDistance(str1,str2,tagCnt):    \n",
    "    tags1 = jieba.analyse.extract_tags(str1, 10)   \n",
    "    tags2 = jieba.analyse.extract_tags(str2, 10)\n",
    "\n",
    "    mtags = tags1 + list(set(tags2) - set(tags1))\n",
    "\n",
    "    dictionary1 = dict(zip(mtags, [str1.count(x) for x in mtags]))\n",
    "    dictionary2 = dict(zip(mtags, [str2.count(x) for x in mtags]))\n",
    "\n",
    "    print(dictionary1)\n",
    "    print(dictionary2)\n",
    "    \n",
    "    d = 1 - distance.cosine(np.array([v for k,v in dictionary1.items()],dtype = object),\n",
    "                           np.array([v for k,v in dictionary2.items()],dtype = object))    \n",
    "    return d\n",
    "\n",
    "s1 = \"狂！TAE成人展「粉紅泡泡浴」最猛畫面曝光！大尺度「挑逗影片」沒有極限…文 / 深夜大主廚 昨天在TAE活動現場的「粉紅泡泡浴」成人表演，引起了現場轟動！這\"\n",
    "s2 = \"好色喔！TAE成人展「暗黑直播女帝」自己脫了！開場讓粉絲「嚐甜頭」畫面好害羞…文 / 深夜大主廚 經過連續三天的「2017 TAE成人博覽會」，主廚深深的體會到Julia、\"\n",
    "\n",
    "print(StrDistance(s1,s2,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1 2]\n",
      " [1 0 2]\n",
      " [2 1 0]\n",
      " [3 4 5]\n",
      " [4 3 5]\n",
      " [5 4 3]]\n",
      "[[ 0.          1.          2.23606798]\n",
      " [ 0.          1.          1.41421356]\n",
      " [ 0.          1.41421356  2.23606798]\n",
      " [ 0.          1.          2.23606798]\n",
      " [ 0.          1.          1.41421356]\n",
      " [ 0.          1.41421356  2.23606798]]\n",
      "[[ 1.  1.  1.  0.  0.  0.]\n",
      " [ 1.  1.  1.  0.  0.  0.]\n",
      " [ 1.  1.  1.  0.  0.  0.]\n",
      " [ 0.  0.  0.  1.  1.  1.]\n",
      " [ 0.  0.  0.  1.  1.  1.]\n",
      " [ 0.  0.  0.  1.  1.  1.]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import NearestNeighbors\n",
    "import numpy as np\n",
    "X = np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]])\n",
    "nbrs = NearestNeighbors(n_neighbors=3, algorithm='ball_tree').fit(X)\n",
    "distances, indices = nbrs.kneighbors(X)\n",
    "print(indices ) \n",
    "print(distances)\n",
    "print(nbrs.kneighbors_graph(X).toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0.95152461221578843: 0.99245439336991648, 0.85375406287000111: 0.8672214150058617, 0.81894177446639138: 0.92321472095784318, 0.97023694739255162: 0.92747403275014961, 0.75611940400390432: 0.78972251279952987, 0.91337497115434163: 0.78562653501063728, 0.82805456161480762: 0.98976473315465796, 0.99396082064266666: 1.038700883468225, 0.81184729762066832: 0.89156092765962458, 0.93814669943247109: 0.95561510174516651, 0.92506458069537034: 0.91431868611618994, 0.92058708606985029: 0.96291131643541505, 0.76535910060334245: 0.88511603839157693, 0.8933055325197764: 1.0033086546828267, 0.9202857342594849: 0.85215491572340807, 0.84512185397859729: 0.74718183950051442, 0.92561772662344666: 1.217176261759074, 0.98496787779291417: 0.89039727407541536, 0.88200668458344311: 0.96796519105403911, 0.74381898721183948: 1.1180855766231463, 0.90821169237883315: 0.9911172303436977, 0.71188337571440397: 1.0996727959603756, 0.72288378135926368: 1.0815146197312859, 0.83349397716329932: 0.86361209938960282, 1.1825406402299599: 0.95844573599440031, 0.9534955440095696: 0.91373369715664376, 0.81964919388948032: 0.99495886806641864, 0.97957389212664259: 0.87055967145467805, 0.97290139085019234: 0.99229743412100069, 0.79404839676727301: 0.81648740876899895, 0.85238531447480814: 1.0574638067010618, 0.98601678126793391: 0.90048145650734346, 0.59837442466741431: 1.0435926290342521, 0.93194323000973533: 0.94075377011467654, 0.77251605918415911: 0.97965588464633158, 0.80578708783171027: 0.99633283429019126, 0.79637083470809922: 0.98012446109405627, 0.86982122094043113: 1.0080934663431604, 0.88096174841216734: 1.0269574566009578, 0.60141270409177616: 0.96744353965271945, 0.88925397766438419: 0.9127065027087109, 0.9178137036590992: 0.95605916075160768, 0.82003764564140724: 1.0251241974076908, 0.73452975207446003: 1.0115746481020922, 0.93386501532900312: 0.99277450410341872, 0.94497438634746078: 1.0025751537644381, 0.93113128109680976: 0.95458348624811917, 0.99897194137114464: 0.95733247545202005, 0.81449642642109421: 0.87932159282219879, 0.96150238077240369: 1.174668245173901, 0.83938017442562352: 0.8025054874014077, 0.75361091490811105: 0.94964101324672412, 0.87840156942956638: 0.98009979567911609, 0.91198307672969736: 0.77192109685170962, 0.97607907124553539: 0.92755742553384513, 0.92929925367393451: 0.81721223584529645, 0.91643348196264118: 0.8896560931159303, 0.85329355968939702: 1.1154397866051911, 0.87065950595801767: 1.0180227417884797, 0.97170467396808036: 1.0476036640065027, 0.84936025265094728: 1.0833040607616535, 0.76132524947555391: 0.87735711087563906, 0.77634410965750422: 0.84051161800878715, 0.98817164056445406: 1.1059334450052054, 0.96160656128744759: 0.91980785737583903, 0.80305899908074752: 0.92445345816713176, 0.74285222910903048: 1.1112753894528666, 0.91871264984067913: 1.0415598660137475, 0.84461770987704521: 0.90832293287256716, 1.1128864055014376: 1.1009906266722091, 0.96549067588681525: 0.97507330943398784, 0.79603292775277601: 1.1174493386837356, 0.82220712335852675: 0.71557137480990307, 0.89312628799841587: 1.0668538545186139, 0.95094359014146956: 0.77814286594007043, 0.96467752748621272: 0.79871709308749261, 0.87664823579504347: 0.97094834450924605, 0.76653738588176878: 0.89050279234887442, 0.98196705373216353: 0.94208035960349223, 0.90402139310912766: 0.99935458888642525, 0.88483620144377562: 1.0532689196751515, 1.0563132665334294: 0.88081376414841017, 0.91921637082510566: 0.96323908208372844, 0.8039532597669945: 0.9250537522167297, 0.80977753271036823: 0.95024368109361945, 0.94438203590741066: 0.99587049218892854, 0.72643606481850875: 0.94108259918276727, 1.039913987295481: 0.84871131447797499, 0.92764664840683086: 0.94837427516138806, 0.98435851394542329: 0.86708078477591044, 0.8639638524307971: 0.98763196233098849, 0.77943044786829463: 1.0600907887108009, 0.84148450681945663: 0.98868345483305986, 0.91809275945206248: 0.99028510026975169, 0.9240656983783867: 1.003759023958511, 0.96104559974897918: 0.93893520635342331, 0.71678726052110331: 1.0169960612967639, 0.94584732279559358: 0.90474895760300744, 0.91545255745788756: 0.94224320589962374, 0.82236252787541853: 0.97864662998717544, 0.59301503186460924: 0.96193159972209719, 1.0482401911729178: 0.90530486947268685, 0.85153174202355375: 0.97761504605962712, 0.8207040217974827: 0.82798059174634731, 0.75144311695184474: 0.98440922830680988, 0.93440688605989353: 0.90121525672875635, 0.86129345962550863: 0.84939708974816142, 1.0952712108252354: 0.8123795823448593, 0.83874783711741052: 0.87846602725934697, 0.85981776702726365: 1.0554136365800773, 0.96868497111706842: 0.98817765718296025, 1.0307201044299215: 1.0469651991766915, 0.69276165839663628: 0.85546326226725522, 0.94641311095212033: 1.0692788318340931, 0.88359603219875238: 0.92670248651757015, 0.87242439696268748: 0.95400464423435438, 0.86539439364409465: 0.80665239351369911, 0.84484008073463657: 0.9514003620586281, 1.1137510862403757: 1.0266763646352597, 0.94259175844610832: 1.0170178114244908, 0.85841024681033162: 0.91744949087791483, 0.81298895854124709: 0.78823220960436147, 0.88233573393793341: 0.96823944083692126, 0.7801995016021559: 0.87547347202368409, 0.91387979313251599: 0.85228603100070854, 0.82246030014045057: 0.89419370207220783, 1.0213340167995404: 0.91186260051878498, 0.94054143137775292: 0.84413596030380345, 0.81106070910621952: 0.83231029869934059, 0.91435516049207555: 1.0202224901102279, 0.7784932090914215: 0.93457085855316446, 0.92829248745443793: 0.90932079996553772, 0.9730800820184996: 0.98815225517037275, 1.0237586650511912: 0.8718115479966948, 0.95293681185281676: 0.83304356586670314, 0.97161630904330587: 0.98866044557088539, 0.96740545725919147: 0.89622354740305987, 0.82025817412088065: 1.0730500389493394, 0.78401566110318788: 1.1600531148997626, 0.8736484277368981: 0.80820385390114535, 0.91626126494293336: 1.0409710537452055, 0.98136002515099807: 0.99478598032439314, 1.0495039759779452: 0.9252846672092564, 0.96942530973524454: 0.88208010651210522, 0.66465073917176609: 0.8865622335429304, 0.86214432040687083: 1.0034360957599948, 0.76513205433566156: 0.84798776908374429, 0.77473201808753311: 0.80190556441466543, 0.79798664006203757: 0.8240359382453778, 0.89536468012749226: 0.86396535659627582, 1.0255028973895455: 0.8625703852081984, 0.87262971892168484: 0.86551710308904761, 0.86197505164980492: 0.95866795651624959, 0.73554435825519837: 0.92402504628366877, 0.83857012514374751: 0.97404474401961993, 0.93539922338520753: 0.90076728004695328, 0.74068465136654482: 1.0464490070266177, 1.0779461610027612: 0.92649094323336212, 0.8800908353283311: 1.0367280625195994, 0.76285898865757584: 0.98968754469340148, 0.85313018653846018: 1.0270751567917769, 0.93247535449037688: 0.94843294698450753, 0.78564710541971416: 0.75490040830484906, 0.96104977426324156: 0.9637855371129791, 0.95748103541311169: 1.0713492429776548, 0.91543018569301848: 0.88959502254551881, 1.1470183118744586: 0.96613882307127608, 0.65800956984431402: 0.81665575198067608, 0.62805549337458222: 0.96165632938126588, 0.65894972106103977: 0.9160980109369351, 0.77285044001419279: 0.90618314672253375, 0.82470783645821877: 0.99643679852805966, 0.85715936547131599: 1.0076065920249571, 0.79421415016367125: 0.98347131818912403, 1.0139210221992252: 1.0521440872687617, 0.86173280015654641: 0.77140512674942685, 0.79032559337320574: 0.73236746300953426, 1.0675273123906235: 0.91795038542717244, 0.88254138300431351: 1.0026520975757722, 0.94182890949015208: 0.92128569293490292, 0.99108092005363069: 0.94524538299605965, 0.96252055300200734: 1.1009938702926538, 0.73816113495469515: 0.9955074542750586, 0.99929376767425682: 0.80332379764598616, 0.62504061895457552: 1.0614572848054247, 0.87522273348594393: 1.067915004166532, 0.79690978246090105: 0.97988276691540088, 0.92482090625575941: 0.92478571557829914, 0.83213682408604439: 0.76922233456880496, 0.88830034932239776: 0.81215006159083536, 0.87305456757652777: 0.83496143325640426, 0.96642705382632987: 0.82557771513212375, 1.0847684195655707: 0.78730190073773543, 0.88341231754185467: 1.0547494553581473, 0.88469827240231602: 0.89882447787778696, 0.83494102578331586: 0.88616198266922597, 0.79778928533318005: 0.97194523795846355, 0.82130750263659458: 0.77542267902505413, 0.90145326367200207: 0.97275690767702194, 0.87497766391873455: 1.0688303546909455, 0.5172517072321281: 0.74121290557606723, 0.57808790965111678: 0.67750388755267865, 0.67857273082739011: 0.75278338624768937, 0.39235335893459156: 0.75363194869725703, 0.44323517079894692: 0.89586219875540318, 0.44806434231116538: 0.77664410192679212, 0.44009998350230373: 0.71060528706392856, 0.47718916624866037: 0.8977760126264912, 0.6351462287194769: 0.68734457618546918, 0.35903212520713501: 0.83227496679169155, 0.29400064498618972: 0.61779149836771086, 0.65808746976072285: 0.90979045008251225, 0.53714882732708968: 0.72855198677514277, 0.37113456708037024: 0.70529754686536661, 0.4288269789971389: 0.92172062665239551, 0.50680953691267516: 0.74362770264103362, 0.52037186786789036: 0.80813100768421908, 0.5705344697901753: 0.70206123704056711, 0.6925375222385135: 0.70361692267845499, 0.37395845666369626: 0.70084685767219534, 0.54463588851345812: 0.7184623888996492, 0.46597430218669333: 0.67012173449862522, 0.45163492528137716: 0.74635403893047381, 0.65951684531448318: 0.80500417276219471, 0.36657201494145319: 0.6867821685925668, 0.59766273333703401: 0.7614220645660309, 0.50227416551640414: 0.79996145256593565, 0.48712766576063005: 0.7944072851715771, 0.54434122029630849: 0.72671569194082875, 0.56315978857038695: 0.82863806505579352, 0.48233284455568903: 0.85702423318672516, 0.51435367349968675: 0.62722264407558903, 0.56309609803361371: 0.7452608429679427, 0.41675055436542652: 0.78211533401083255, 0.53304282292967964: 0.73774282962888638, 0.54580945980663331: 0.73302335940300389, 0.5755737163138821: 0.60078886765266681, 0.35727400287249966: 0.88036128433253713, 0.59863387482132968: 0.76463218946188272, 0.59762110231396226: 0.6932573143640326, 0.44647584354346009: 0.82038354273654479, 0.55070234310967703: 0.80823396191844843, 0.18254064022995969: 0.83274267170146077, 0.59872993718797496: 0.58986142043616963, 0.5249354013102332: 0.7466626626589199, 0.43235285070647927: 0.82216442207697804, 0.48402109397696996: 0.60964820755159677, 0.46348677146598111: 0.67185620662433376, 0.48313541299898621: 0.68304151948127523, 0.63923265481428404: 0.62240325000698882, 0.64647379224642454: 0.77796630533906319, 0.55888174258716394: 0.70368936434190665, 0.36161410883178524: 0.78810204472045708, 0.43368522045790875: 0.70266873460063661, 0.32920123948336211: 0.74307556859373514, 0.52767738718277191: 0.76259057641632277, 0.48388186831906471: 0.76750610893426663, 0.45661209096985172: 0.79282881667868776, 0.52078826176257265: 0.86309732458998323, 0.51946042258170366: 0.85190875216043294, 0.50121488127319991: 0.76001479699398622, 0.58378595429993585: 0.52058100299118293, 0.4469622157046757: 0.74877697795348097, 0.57322363132204046: 0.66899043691054483, 0.50779738810379516: 0.7195814726629135, 0.3545540393215168: 0.6240762069507122, 0.54701811108336007: 0.64988672759672372, 0.42463767954215736: 0.60028818179431354, 0.55331270098256558: 0.79720038395628001, 0.49121648299242071: 0.84686633804840805, 0.32135402404695029: 0.74816458900972571, 0.50247158275339943: 0.61494684182034653, 0.50530915447225144: 0.74891228885228511, 0.37085447227725465: 0.76852981483104754, 0.51673374014524798: 0.72014418799361757, 0.63117682249160545: 0.82781253840973512, 0.50423782549414087: 0.70136821001525484, 0.38973993651588323: 0.67433022736035986, 0.5309293483594637: 0.61972645587725539, 0.5525802973816698: 0.76394077768008029, 0.56215991226506379: 0.85433969009915744, 0.65515864611384023: 0.7130400255766578, 0.41992333566177498: 0.85514417847537727, 0.55556187842000337: 0.85833344423873992, 0.66305506223453636: 0.73778691182921996, 0.58888098646140463: 0.74090875458673155, 0.47828804140250009: 0.74130341380170306, 0.40419334105293148: 0.79130004265655962, 0.27373192506068039: 0.7310190961618761, 0.58357002209210507: 0.60356191046237906, 0.40725004890517563: 0.90586422247070697, 0.55569651286094868: 0.70748382026817669, 0.58731877474399563: 0.66711279263343626, 0.3177147703690732: 0.66745352997087537, 0.56527562151899102: 0.69933015074105676, 0.41121653287302817: 0.80070443004229108, 0.43166021043029729: 0.75012922728696663, 0.40279599016371143: 0.82794287576681369, 0.55637590374040091: 0.73966870666061768, 0.68724938698900506: 0.64928104703467682, 0.5554041574338382: 0.66667321480380315, 0.42526318992758738: 0.7458375878523642, 0.57842014990951029: 0.78692962445866088, 0.45397588252170029: 0.61246678974309787, 0.61075560149304753: 0.85574301240186057, 0.57050145056272383: 0.74070126023258065, 0.49980026705426261: 0.60588623146130816, 0.51580045359390791: 0.64068294347309618, 0.55220081404982713: 0.80900891965962496, 0.45875756747012142: 0.75130333307868846, 0.31452557692476435: 0.72298294415690134, 0.62621705571433217: 0.59318306635809503, 0.24192801512546011: 0.76605831627085119, 0.42428490337090891: 0.73744541299867084, 0.45709876719468578: 0.91161422834368555, 0.53614144827224375: 0.63905825183533926, 0.51819215795857887: 0.77870832565720349, 0.50258498364583903: 0.70458714442869286, 0.44665346991591154: 0.78601738930092624, 0.49733746189838607: 0.82072391645565013, 0.64654787610601838: 0.7695561529358822, 0.4535667595404313: 0.68739693613311781, 0.47497632642943283: 0.74696557661010343, 0.46582529704434533: 0.75284071136138797, 0.49285025971657281: 0.84701945095964526, 0.53362064901823258: 0.75269504848361579, 0.56348101930679451: 0.74063264565594766, 0.48189944676469088: 0.68703820881498112, 0.42528995895582461: 0.83873209527091785, 0.55023849956301207: 0.79199453549819532, 0.56333906623038554: 0.78192032119110266, 0.46018948399590121: 1.0010273924913522, 0.65540275939722659: 0.80923377863134482, 0.66981650248688995: 0.72539064837295719, 0.58147870451947592: 0.90123038887196927, 0.38836047627959719: 0.73203085539386148, 0.40928660264511513: 0.72936062729105078, 0.22001932078623695: 0.64534277347904234, 0.55613990501133204: 0.82708232417587357, 0.40460233473786539: 0.71488935732055314, 0.50081704188357434: 0.86207048262797981, 0.424243011157383: 0.7190594207412655, 0.41640726660905863: 0.62470547996918946, 0.66326863864988683: 0.80294087375959422, 0.27424195007713842: 0.7665469042325791, 0.4932952905484933: 0.73854780731779557, 0.44989620700858418: 0.85197091402597969, 0.39580637366966531: 0.70282059561899679, 0.50696026864492671: 0.52190631828363232, 0.53825838027389095: 0.61218891436418266, 0.57744367838912325: 0.71853643480003404, 0.54441401211540741: 0.76437054825591877, 0.46413746651825399: 0.80381799124299824, 0.54529871890824078: 0.5150825510743583, 0.65964681411665793: 0.76887407770079064, 0.58181644583238556: 0.86062740858654607, 0.49608778869330555: 0.86307453555045888, 0.32554548755182422: 0.81399089242498202, 0.70482982863942178: 0.65890563311866468, 0.64174006136481687: 0.81737376908843118, 0.48863022409948331: 0.62684247741624022, 0.4970660778719197: 0.70136439728971955, 0.67415908530242308: 0.90216089327755644, 0.5597090565301055: 0.63394286326067895, 0.60470627625367679: 0.78515224197461309, 0.30492001347747194: 0.66032361098037495, 0.46290565967180319: 0.91273233050682534, 0.53631896175663674: 0.71917189649058444, 0.74842239133548838: 0.72452000177661058, 0.48908554177505531: 0.71582661881849541, 0.60066035311264387: 0.73609086349130681, 0.65650678585069311: 0.72968816890528609, 0.45463503093067481: 0.70456754779648867, 0.37702552975989506: 0.64752153338537743, 0.4107158563637569: 0.73144276932059227, 0.43433258921720486: 0.61843606015149333, 0.50442684610696564: 0.69970549952091243, 0.50631331238532329: 0.83217236911997516, 0.52096112349510115: 0.71016835020269353, 0.40789249098962577: 0.86599312051717536, 0.49725130917445393: 0.80761130517831503, 0.52203518796641657: 0.6995318282997377, 0.76829231001095022: 0.77497668928010921, 0.45352798945275891: 0.77596903345181634, 0.71475540275137861: 0.76383802206114626, 0.53942686512517324: 0.89958213243679519, 0.48203680112121122: 0.82047140624217929, 0.49422048254715295: 0.81448907163706441, 0.53438540276837343: 0.95669516015656419, 0.57518224142441632: 0.8244658962743967, 0.64395146059682429: 0.70922280339923882, 0.55224163387413627: 0.76390692027713569, 0.56899189568206865: 0.72502906044362403, 0.74501028430295746: 0.7621429803488371, 0.54152966531049229: 0.70189117115150002, 0.59614381163102403: 0.86312528611023698, 0.43447023879435342: 0.67910773149389791, 0.54725398929054003: 0.6852960864314821, 0.50908099226100789: 0.70320376743442248, 0.70430475122112712: 0.76507171751936776, 0.86569841084691423: 0.51972533616757033, 0.73698472539001514: 0.21717626175907398, 0.72202577897390419: 0.56445672760386523, 0.81126679938871038: 0.50903099878772007, 0.84769195342866455: 0.45241744820458779, 0.94881180560717504: 0.39246045876864527, 0.77096736076279926: 0.41450377414578748, 0.71017809646524432: 0.46949256989103094, 0.92253382369350523: 0.38390908340423208, 0.75436496850808632: 0.51698281482915243, 0.712025225439192: 0.62677976795663037, 0.87053915315751984: 0.39679224207887476, 0.99637021293940164: 0.37251938034785448, 0.5787657201196944: 0.38854164813285674, 0.70315370417978185: 0.486421633147049, 0.70722071685027355: 0.42234724940585361, 0.7921286546558729: 0.58832120347774486, 0.83379197788979564: 0.47339635180578832, 0.56524571374587185: 0.37024409328512126, 0.86305551810447001: 0.46697968576140975, 0.93243609971652353: 0.53349881468909555, 0.65927675093998206: 0.35167302882466295, 0.73393606725428728: 0.33784306723176061, 0.77222557391785163: 0.5473560673693415, 0.85696164986328216: 0.38123385265423088, 0.82737760859003318: 0.41934038634586157, 0.77573454918156437: 0.43420631951879385, 0.84097541616958593: 0.33116309533213251, 0.65996917853797898: 0.44685654520158541, 0.65322595751361168: 0.45973075777992484, 0.52390728810210663: 0.42127214971802873, 0.67353707979433097: 0.34540448239066762, 0.82317219936408736: 0.46309065559816592, 0.87083108752513361: 0.51681282981624443, 0.93588673646435872: 0.58658320887376669, 0.9588516246874752: 0.59270845753151991, 0.85490115456443183: 0.44631320943619834, 0.67481652148665272: 0.41675717685885655, 0.81924415436554188: 0.48162371822061939, 0.8586831925224141: 0.42253061991542024, 0.84852656711335739: 0.45088820609675379, 0.83101091627414581: 0.44277532855248602, 0.77434382858797512: 0.49406380212380147, 0.88763779622610739: 0.50575593920090445, 0.794967995695264: 0.40753276328783244, 0.74438235928992735: 0.57598849447737654, 0.72391461475210217: 0.56531580920821367, 0.84707342661540375: 0.31907169234976107, 0.73909653371609829: 0.33918856652730556, 0.98622790732723531: 0.34601340485944582, 0.62741685963453353: 0.45959727269276868, 0.57429838391877008: 0.44002863439822537, 0.82793543548039916: 0.40236927966062286, 0.7778750872245942: 0.43684829055431967, 0.78958065933144117: 0.5512770638657617, 0.8923836435031427: 0.30112506621119839, 0.73371631397317283: 0.54592490790403703, 0.65439624593411561: 0.34183409091935707, 0.72817097591744939: 0.56977316248402687, 0.90559471350683718: 0.36130684843411209, 0.95061536276082848: 0.53218311371817961, 0.79987917147572107: 0.42172394171832384, 0.85365102254004144: 0.59077811184835505, 0.98509572330237993: 0.50984369656256967, 0.84011128155839587: 0.5348639357164594, 0.76386576377289694: 0.54668582559610301, 0.72783109805753154: 0.58654314615085845, 0.89013885439457829: 0.57940940008109743, 0.8172845350969048: 0.47340749159687401, 1.0266035312709585: 0.37898492630730429, 0.63073872427993516: 0.49988939762054801, 0.79375468212202105: 0.33284586095740348, 0.80240378264421575: 0.51494890958640827, 0.75886197465446126: 0.40506782084077109, 0.68647967037408353: 0.33059358845418618, 0.8337456570593208: 0.3651951227850303, 0.95725299173909362: 0.46931647297326473, 1.0698133017933473: 0.48514426319689508, 0.84317255417767611: 0.36387811725272429, 0.73985558635147686: 0.50777156939033052, 0.87115443965930706: 0.63839978312063639, 0.64493100761397093: 0.49779376219435012, 0.8543119225410466: 0.3233748562372471, 0.76836458950771536: 0.44515412757091988, 0.72204191234123349: 0.26179319335304352, 1.040409789092507: 0.5498666831847413, 0.78864365451986151: 0.43173573310493307, 0.87096074991084771: 0.26419701886683533, 0.65125780778173259: 0.48744612386729058, 0.78681958927710371: 0.41967743275765168, 0.90623443013879246: 0.47593633707622351, 0.74414497429210658: 0.51602599947235139, 0.67028932312077205: 0.44258750258190466, 0.86070211180260214: 0.53319016909629602, 0.82037259031162846: 0.38843268008127924, 0.81020568164993334: 0.4182333442561556, 0.93745770545919138: 0.43477212166728785, 0.92539988692585995: 0.30954583629070415, 0.6710388944498249: 0.59112420171263291, 0.88465283057699629: 0.43971640757419844, 0.83563683574622261: 0.44674130655366873, 0.74543384323819217: 0.62233904970987808, 0.84637367222607296: 0.39966811959369486, 0.63915963048847124: 0.50637969035666341, 0.68753903654130477: 0.44007158551038417, 0.93439125523654931: 0.68332920397759966, 0.90151225090517295: 0.46135704589094367, 0.69321822780047759: 0.33689553021534469, 0.72291296452259768: 0.56144514725840788, 0.82795789333044378: 0.33181413443487046, 0.83093761584386205: 0.45024786558452246, 0.72472737756721062: 0.52931168669713624, 0.93364731501077147: 0.40571117735491258, 0.71184153087059499: 0.43524650290098083, 0.69974112307689884: 0.4992286941136353, 0.62964351209124914: 0.34404691948442351, 0.70533944852495845: 0.48270060873765319, 0.87071786486188119: 0.38698133626632147, 0.70170018872170392: 0.47576930335295592, 0.75921042938793526: 0.60610121002404049, 0.63653011615431898: 0.48222707598717984, 0.73931709024733994: 0.4752472711441364, 0.90374588575855441: 0.3124050326079702, 0.78233706338106335: 0.61992909578890065, 0.69024856432139081: 0.62238630698485442, 0.78759117871192774: 0.61826549841967771, 0.99060390565587386: 0.39466267391705906, 0.76113338627312543: 0.64373095986676832, 0.65339443984514278: 0.35104259253116105, 0.62838337496665675: 0.42640863502062964, 0.83669729217280953: 0.4485505842438739, 0.50613154154206419: 0.5211375710062478, 0.92699132382300042: 0.39183392673838552, 0.88953210649891568: 0.45470645639852986, 0.84116558872340785: 0.45090798022104672, 0.72659981651862715: 0.62675757650500741, 0.97038119940516387: 0.38401619815639559, 0.88262505996164742: 0.54219468463432008, 0.92694371842281109: 0.38070021496156725, 0.80999392534199011: 0.48546695842753013, 0.80614103239490109: 0.62460320076667697, 0.69800874448151018: 0.40077451435970723, 0.8036080032378462: 0.51063337047273005, 0.68582326055677378: 0.55269454798126472, 0.83802237454017736: 0.41497987374449175, 0.79071069777952874: 0.55821171432366845, 0.80133287099425143: 0.47119981540671751, 0.77875584449141888: 0.39361458951670603, 0.70493780893617541: 0.52943610904890281, 0.83085893628753205: 0.54664364385589759, 0.58065754164906924: 0.48630688230713803, 0.80294254074958438: 0.46609567388292211, 0.71366191809306112: 0.48421619586926717, 0.6138305710870775: 0.42483623312016672, 0.8675938431654806: 0.35494039838577474, 0.8156820541648645: 0.60421033770533805, 0.89597732993382073: 0.56521501024955456, 0.74676720304681088: 0.4523525336874239, 0.75101809998021807: 0.41497678068831834, 0.77760157176030964: 0.70668722768178793, 0.838539348854898: 0.46095711306220943, 0.88523050378359058: 0.37763786098140212, 0.75715439451932076: 0.59084295035868439, 0.84856558194406229: 0.3613504928718948, 0.67379499605119575: 0.45046678420299607, 0.77550175427813883: 0.34985856657754527, 0.76544856713234222: 0.4498395373908266, 0.79785534363149158: 0.53609245644929426, 0.79502086297969965: 0.57898375673851199, 0.64828955427511692: 0.58624435404369646, 0.89695995251442651: 0.414679315578384, 0.84839361229296995: 0.46152351483865939, 0.81595105371584564: 0.45996875071131788, 0.95002516846096352: 0.47029590615963862, 0.71938143883695926: 0.4985528147448865, 0.71112141811024954: 0.25213416534203403, 0.56977793423780032: 0.35311532640579057, 0.71350526493969824: 0.49400694893848318, 0.67754902538491402: 0.32816584563564316, 0.63879707063511915: 0.40214766605454305, 0.86698820918442498: 0.29338849869224909, 0.71213244679676102: 0.52788904633391931, 0.93138895937000887: 0.52797891900424732, 0.68759652914668201: 0.475830197695224, 0.85555885801020315: 0.32283918061071254, 0.67422496889045014: 0.60988357873554544, 0.80012547385957078: 0.59399589318081925, 0.79612154099560573: 0.42894372953114818, 0.92408335027335675: 0.55864553715137266, 1.0372309872853858: 0.54255992672364128, 0.82556574577084485: 0.43472083970378322, 0.90131422929043381: 0.39127796931259079, 0.9252850304615734: 0.29771033674826264, 0.88650843924500999: 0.46944332235266539, 0.66908563325577741: 0.42713002425707469, 0.87060724515583721: 0.45896837857550415, 0.98115934390714477: 0.34223436942361157, 0.77398539494826102: 0.60725463370682842, 0.59468294926941867: 0.60877175875901368, 0.84533243683069259: 0.48435534800766777}\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "from sklearn import neighbors\n",
    "\n",
    "x1 = np.random.normal(50, 6, 200)\n",
    "y1 = np.random.normal(5, 0.5, 200)\n",
    "\n",
    "x2 = np.random.normal(30,6,200)\n",
    "y2 = np.random.normal(4,0.5,200)\n",
    "\n",
    "x3 = np.random.normal(45,6,200)\n",
    "y3 = np.random.normal(2.5, 0.5, 200)\n",
    "\n",
    "x_val = np.concatenate((x1,x2,x3))\n",
    "y_val = np.concatenate((y1,y2,y3))\n",
    "\n",
    "x_diff = max(x_val)-min(x_val)\n",
    "y_diff = max(y_val)-min(y_val)\n",
    "\n",
    "x_normalized = [x/(x_diff) for x in x_val]\n",
    "y_normalized = [y/(y_diff) for y in y_val]\n",
    "xy_normalized = zip(x_normalized,y_normalized)\n",
    "\n",
    "print(dict(xy_normalized))\n",
    "\n",
    "labels = [1]*200+[2]*200+[3]*200\n",
    "\n",
    "clf = neighbors.KNeighborsClassifier(30)\n",
    "\n",
    "?clf.fit(xy_normalized, labels)\n",
    "\n",
    "#nearests = clf.kneighbors([(50/x_diff, 5/y_diff),(30/x_diff, 3/y_diff)], 10, False)\n",
    "#nearests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.11022302463e-16\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import pairwise_distances\n",
    "from scipy.spatial.distance import cosine\n",
    "from scipy.spatial import distance\n",
    "import numpy as np\n",
    "\n",
    "A = np.array([[1,23,2,5,6,2,2,6,2],[12,4,5,5],[1,2,4],[1],[2],[2]], dtype=object )\n",
    "B = np.array([[1,23,2,5,6,2,2,6,2],[12,4,5,5],[1,2,4],[1],[2],[2]], dtype=object )\n",
    "\n",
    "Aflat = np.hstack( A )\n",
    "Bflat = np.hstack( B )\n",
    "\n",
    "dist = distance.cosine( Aflat, Bflat )\n",
    "print(dist)\n",
    "\n",
    "\n",
    "print(distance.cosine(np.array([1,2,3,4,5,6],dtype = object),\n",
    "                       np.array([1,2,3,4,5,6],dtype = object)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  -6.91850838    2.84800453    1.24254175   -5.23056944    3.80596305]\n",
      " [  -6.91850838    2.84800453    1.24254175   -5.23056944    3.80596305]\n",
      " [  -6.91850838    2.84800453    1.24254175   -5.23056944    3.80596305]\n",
      " [  -6.91850838    2.84800453    1.24254175   -5.23056944    3.80596305]\n",
      " [  -6.91850838    2.84800453    1.24254175   -5.23056944    3.80596305]\n",
      " [  94.39895422   96.89373003  108.15437007  101.0373811    99.17597179]\n",
      " [  94.39895422   96.89373003  108.15437007  101.0373811    99.17597179]\n",
      " [  94.39895422   96.89373003  108.15437007  101.0373811    99.17597179]\n",
      " [  94.39895422   96.89373003  108.15437007  101.0373811    99.17597179]\n",
      " [  94.39895422   96.89373003  108.15437007  101.0373811    99.17597179]]\n",
      "predictX [100, 100, 100, 100, 100]\n",
      "predict result: ['類別2']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/sklearn/utils/validation.py:395: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'predicted' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-67-ff509de7d845>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'predict result:'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclassification_report\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredicted\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'predicted' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "from sklearn import svm\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# fit a SVM model to the data\n",
    "\n",
    "size = 5\n",
    "x1 = np.random.normal(1, 6, size)\n",
    "x2 = np.random.normal(100, 6, size)\n",
    "\n",
    "x_val = np.concatenate((x1,x2))\n",
    "\n",
    "\n",
    "x_val = [np.random.normal(1, 6, size)] * 5\n",
    "x_val1 = [np.random.normal(100, 6, size)] * 5\n",
    "\n",
    "x_val = np.concatenate((x_val,x_val1))\n",
    "print(x_val)\n",
    "\n",
    "y_val = ['類別1'] * size + ['類別2'] * size\n",
    "\n",
    "clf = svm.SVC(decision_function_shape='ovo')\n",
    "clf.fit(x_val, y_val) \n",
    "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
    "    decision_function_shape='ovo', degree=3, gamma='auto', kernel='rbf',\n",
    "    max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
    "    tol=0.001, verbose=False)\n",
    "\n",
    "predictX = np.random.normal(1, 6, size)\n",
    "predictX = [100] * size\n",
    "print('predictX',predictX)\n",
    "print('predict result:',clf.predict(predictX))\n",
    "\n",
    "print(metrics.classification_report(y_val, predicted))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n",
      "4\n",
      "[2]\n"
     ]
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "X = [[0], [10], [20], [30]]\n",
    "Y = [0, 1, 2, 3]\n",
    "clf = svm.SVC(decision_function_shape='ovo')\n",
    "clf.fit(X, Y) \n",
    "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
    "    decision_function_shape='ovo', degree=3, gamma='auto', kernel='rbf',\n",
    "    max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
    "    tol=0.001, verbose=False)\n",
    "\n",
    "dec = clf.decision_function([[1]])\n",
    "print(dec.shape[1]) # 4 classes: 4*3/2 = 6\n",
    "\n",
    "clf.decision_function_shape = \"ovr\"\n",
    "dec = clf.decision_function([[1]])\n",
    "print(dec.shape[1]) # 4 classes\n",
    "\n",
    "print(clf.predict([[20.]]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
